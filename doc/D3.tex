%
% File acl2014.tex
%
% Contact: koller@ling.uni-potsdam.de, yusuke@nii.ac.jp
%%
%% Based on the style files for ACL-2013, which were, in turn,
%% Based on the style files for ACL-2012, which were, in turn,
%% based on the style files for ACL-2011, which were, in turn, 
%% based on the style files for ACL-2010, which were, in turn, 
%% based on the style files for ACL-IJCNLP-2009, which were, in turn,
%% based on the style files for EACL-2009 and IJCNLP-2008...

%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt]{article}
\usepackage{acl2014}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{graphicx}


%\setlength\titlebox{5cm}

% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.


\title{Ling 573 Project: Multi-Document Summarization}

\author{Emma Bateman \\
  {\tt ebateman@uw.edu} \\\And
  John Dodson \\
  {\tt jrdodson@uw.edu} \\\And
  Charlie Guo \\
  {\tt qcg@uw.edu}
  }

\date{}

\begin{document}
\maketitle
\begin{abstract}
We describe an unsupervised multi-document summarization system using TextRank, a graph-based algorithm for ranking salient sentences in a corpus of text documents. The most salient sentences are selected and then re-ordered using a local coherence algorithm for modeling entity distributions across the input texts. The system is divided into four components for selecting content, generating feature representations, ranking, and ordering.
\end{abstract}

\section{Introduction}

Multi-document summarization systems seek to aggregate collections of text documents and condense their content to provide cohesive summaries. Traditional approaches to summarization fall into two distinct categories: abstractive and extractive. Abstractive techniques are categorized based on their ability to generate novel summary content, in contrast with extractive approaches which select significant content directly from the source documents. In this work, we present an unsupervised system for extractive multi-document summarization over a benchmark dataset. The system is centered around the TextRank algorithm, which is a graph-based technique to identify and rank the most salient sentences in the input documents. We evaluate our system on the AQUAINT and AQUAINT-2 corpora, with average ROUGE-1 and ROUGE-2 F-scores of $0.26093$ and $0.06952$, respectively. 

This report is structured as follows: Section 2 provides an overview of our system and its components, including a description of the AQUAINT dataset; section 3 details our approach to generating summaries using TextRank and entity-grid representations; section 4 presents result metrics; section 5 is a discussion of the approach and details potential improvements to the baseline system; section 6 concludes.

\section{System Overview}
The system is divided into three primary components: a content selector, a ranker, and a sentence ordering algorithm. The content selector is responsible for ingesting a schema file which details one or more document clusters in the AQUAINT datasets. The selector identifies which documents belong together in a cluster and then ingests those documents as plain text. The system extracts feature vectors from each sentence by computing word frequencies. The feature vectors are given as input to the ranking algorithm which runs TextRank to sort vectors by salience. Ranked sentences undergo deduplication and the system selects the top-K salient sentences as input to an entity-based representation mechanism which is used to model coherence and improve human readability.

\subsection{Architecture}
\begin{figure}
  \includegraphics[width=\linewidth]{arch.PNG}
  \caption{Multi-document summarization system architecture}
  \label{fig:arch}
\end{figure}

Figure \ref{fig:arch} provides an illustration of the existing system architecture.

\subsection{AQUAINT Dataset}
We use the AQUAINT (Advanced Question-Answering for Intelligence) and AQUAINT-2 corpora to provide input to our system. Both datasets are composed of English newswire texts. For the multi-document summarization task, we leverage a corresponding schema file which defines one or more document-topic clusters. 

\section{Approach}

This section details the main components of the working system.

\subsection{Content Selection}
The ingest processor for the system consumes one or more documents associated with a cluster identifier. These SGML documents are converted to plain text and represented as a single monolithic text string. After ingest is complete, the system will maintain one text representation per document cluster.

The system uses BeautifulSoup to perform most of its SGML and XML parsing. For a given input file, the ingest processor identifies clusters and their associated documents by finding document IDs which correspond to AQUAINT SGML files. The system then loads the corresponding SGML files and pefrorms traversal using BeautifulSoup to find the appropriate blocks of text.
\subsubsection{Feature Vectors}
The feature extraction component takes the plain text representation and tokenizes it into individual sentences using Python's Natural Language Toolkit (NLTK). NLTK is used to remove common stopwords from each sentence. Feature vectors are generated in two steps: firstly, the component identifies a comprehensive vocabulary for the cluster; and secondly, a frequency vector is created based on the token frequencies in the given sentence. 

The output of this component is an $N$ x $V$ matrix, where $N$ is the total number of sentences in the cluster and $V$ is the size of the vocabulary. 
\subsubsection{TextRank}
Our system uses TextRank to compute saliency scores for each sentence in the cluster and subsequently rank them according to significance. The TextRank algorithm constructs a graphical representation of textual features, where a given textual feature is represented as a vertex in the graph. The algorithm assigns a significance score to each vertex based on inbound edge weighting.

The authors of the TextRank algorithm formally define a graph as a tuple $G = (V,E)$, where $V = \{v_1, v_2, ... v_n\}$ is the set of vertices in the graph and $E$ is a set of edges and a subset of $V$ x $V$. TextRank is inspired by PageRank and computes the score for some vertex $V_i$ similarly to the original algorithm:
\begin{center}
    $S(V_i) = (1 - d) + d * \sum_{j \in In(V_i)} \frac{1}{|Out(V_j)|}S(V_j)$
\end{center}
$In(V_i)$ represents all inbound connections to vertex $V_i$, and $Out(V_j)$ represents outbound connections from vertex $V_j$. The authors use $d$ as a damping factor set between $0$ and $1$, which effectively biases the calculation. The original PageRank algorithm defaults this factor to $0.85$, and in our system we maintain this defaulted value.

In applying TextRank to sentence processing, we compute a pairwise similarity matrix of size $N$ x $N$, where $N$ is the total number of sentences in the cluster. The similarity between $N_i$ and $N_j$ becomes the edge weight which connects those two vertices in the graph. We use cosine similarity as our distance measurement.

After the TextRank algorithm converges, the algorithm returns the top K sentences as the cluster summary. Each summary contains 100 words maximum. The system maintains a 
reference to the original sentences, and uses the original text as output.

\subsubsection{Redundancy}
In our earlier system, we encountered significant amounts of redundancy, including repeated sentences. In one case, a sentence appeared in a summary three times in a row. 

\begin{quote}
``The nation deserves and I will select a Supreme Court justice that Americans can be proud of," Bush said.\\
``The nation deserves and I will select a Supreme Court Justice that Americans can be proud of," Bush said.\\
``The nation deserves and I will select a Supreme Court justice that Americans can be proud of," Bush said.\\
Sandra Day O'Connor, the first  woman ever appointed to the US Supreme Court, said Friday that she is retiring, giving US president George W. Bush his first  opportunity to appoint a justice.
\end{quote}

We fixed this issue by modifying the TextRank algorithm. At each iteration, until enough sentences have been selected to complete the summary, the most salient sentence is extracted. Then, the corresponding feature vector is subtracted from the rest of the matrix, disadvantaging sentences with a high level to similarity to the selected sentence. Finally, the similarity algorithm is rerun before the next sentence is selected.\\

\subsection{Information Ordering}
The system uses an entity-based modeling approach to improve summary coherence and provide a more logical ordering of the sentences in each summary. Our approach is inspired by the entity grid mechanism introduced in Barzilay and Lapata 2008, wherein the authors describe a local coherence algorithm centered around entity transitions. Our algorithm models the distribution of entities across the top outputs of TextRank, which means the input data is fundamentally small. We make the assumption that the input sentences naturally contain local coherence after applying TextRank. 

\subsubsection{Extracting Grammatical Roles}
The authors of the original algorithm exploit several factors in their entity representations, namely, coreference resolution, grammatical roles, and saliency. Our system only uses grammatical function in determining entity types. We use the Stanford CoreNLP toolkit to generate dependency parses over each input sentence. Subject and object relations are extracted from the parse tree and used to model entity transitions. We provide weights for entities with subject, object, and alternate relation categories.

\subsubsection{Entity Grid Representation}
Following the conventions established in Barzilay and Lapata 2008, we use the grammatical roles described in Section 3.2.1 to denote types of discourse entities. After generating dependency parses and compiling the set of unique entities in the input corpus, we compile a table representation of size $N$ x $M$, where $N$ is the total number of sentences in the input corpus and $M$ is the total number of unique entities extracted from that corpus. In our system $N$ is reasonably small.

The entity grid is populated with weight values corresponding to the grammatical function of an entity in a given sentence, where a table entry $E_{i,j}$ is weighted depending on the grammatical role of entity $j$ in sentence $i$. 

\subsection{Baselines}
In addition to the entity grid algorithm, we implemented three baseline ordering algorithms.

\subsubsection{Saliency Order}
The saliency baseline outputs sentences in the same order that they were returned by TextRank.\\

\subsubsection{Original Document Order}
For the original order baseline system, the position of a sentence in a summary is determined by its placement in its original document.\\

During the feature extraction stage, a number is generated for each sentence to represent the position of the sentence in the document. This number is calculated by dividing the number of preceding sentences by the total number of sentences in the document.\\

After TextRank is run, the selected sentences are sorted by their position values to form the final summary.\\

\subsubsection{Traveling Salesman Order}
The "traveling salesman" baseline system orders the sentences by solving the traveling salesman problem for the feature vectors of the selected sentences.\\

The distances between vectors are calculated using cosine similarity.\\

The sentences are output in the order of the shortest path, starting with the most salient of the two endpoints.\\

\section{Results}

\begin{table}[ht]
\caption{Training set ROUGE-1 scores}
\centering
\begin{tabular}{c c c c}
\hline
Algorithm & Recall & Precision & F1\\
Saliency & 0.22170 & 0.28085 & 0.24551\\
Doc Order & 0.22170 & 0.28085 & 0.24551\\
Salesman & 0.22170 & 0.28085 & 0.24551\\
	Entity Grid & 0.22170 & 0.28085 & 0.24551\\
\end{tabular}
\end{table}

\begin{table}[ht]
\caption{Training set ROUGE-2 scores}
\centering
\begin{tabular}{c c c c}
\hline
Algorithm & Recall & Precision & F1\\
Saliency & 0.05605 & 0.07022 & 0.6184\\
Doc Order & 0.05614 & 0.07035 & 0.06195\\
	Salesman & \textbf{0.05624} & \textbf{0.07045} & \textbf{0.06205}\\
	Entity Grid & 0.05595 & 0.07012 & 0.06174\\
\end{tabular}
\end{table}

\begin{table}[ht]
\caption{Development set ROUGE-1 scores}
\centering
\begin{tabular}{c c c c}
\hline
Algorithm & Recall & Precision & F1\\
	Saliency & 0.21358 & 0.27155 & 0.23722\\
	Doc Order & 0.21358 & 0.27155 & 0.23722\\
	Salesman & 0.21358 & 0.27155 & 0.237322\\
	Entity Grid & 0.21358 & 0.27155 & 0.23722\\
\end{tabular}
\end{table}

\begin{table}[ht]
\caption{Development set ROUGE-2 scores}
\centering
\begin{tabular}{c c c c}
\hline
Ordering Algorithm & Recall & Precision & F1\\
	Saliency Order & 0.5326& 0.06801 & 0.05927\\
	Original Document Order & 0.05331 & 0.6809 & 0.05932\\
	Traveling Salesman & \textbf{0.05335} & \textbf{0.06814} & \textbf{0.05937}\\
	Entity Grid & 0.05326 & 0.06801 & 0.05927\\
\end{tabular}
\end{table}

\section{Discussion}
Problematically, there was little difference between our various sorting algorithms. Part of the problem is that the algorithms had to work with what they were given; often, there was no good way to organize the selected sentences.\\
A big issue with our current system is that it lacks sufficient preprocessing and postprocessing. By incorporating more linguistic information in our feature vectors and transforming the selected sentences during content realization, we hope to make significant improvements in our next iteration.\\


\section{Conclusion}
 We have described an unsupervised system for generating single summaries over multiple documents. The system generates feature vectors per sentence by computing word frequencies, and then provides those frequency vectors as input to the TextRank algorithm. The algorithm is able to identify significant lines of text in a single pass. We take the top K sentences as the corresponding summary, up to 100 words max per summary.

% include your own bib file like this:
%\bibliographystyle{acl}
%\bibliography{acl2014}

\begin{thebibliography}{}

\bibitem{textrank04}
	Rada Mihalcea and Paul Tarau,
	\textit{TextRank: Bringing Order into Texts},
	Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing,
	2004.

\bibitem{entitygrid08}
    Regina Barzilay and Mirella Lapata,
    \textit{Modeling Local Coherence: An Entity-Based Approach}
    Computational Linguistics, 
    2008

\bibitem{corenlp}
    Christopher Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven Bethard, and David McClosky,
    \textit{The Stanford CoreNLP Natural Language Processing Toolkit},
    Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations
\end{thebibliography}

\end{document}
